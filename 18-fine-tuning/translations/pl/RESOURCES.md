# Zasoby Do Samodzielnej Nauki

Ta lekcja została zbudowana w oparciu o szereg podstawowych zasobów od OpenAI i Azure OpenAI jako źródła terminologii i samouczków. Oto niewyczerpująca lista, dla Twoich własnych samodzielnych podróży edukacyjnych.

## 1. Podstawowe Zasoby

| Tytuł/Link                                                                                                                                                                                                                      | Opis                                                                                                                                                                                                                                                                                                                         |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Fine-tuning z Modelami OpenAI](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                                           | Fine-tuning doskonali uczenie się na kilku przykładach, trenując na znacznie większej liczbie przykładów niż może zmieścić się w prompcie, oszczędzając koszty, poprawiając jakość odpowiedzi i umożliwiając żądania o niższym opóźnieniu. **Uzyskaj przegląd fine-tuningu od OpenAI.**                                      |
| [Czym jest Fine-Tuning z Azure OpenAI?](https://learn.microsoft.com/azure/ai-services/openai/concepts/fine-tuning-considerations#what-is-fine-tuning-with-azure-openai?WT.mc_id=academic-105485-koreyst)                        | Zrozum **czym jest fine-tuning (koncepcja)**, dlaczego warto się nim zainteresować (motywujący problem), jakie dane wykorzystać (trenowanie) i mierzenie jakości                                                                                                                                                             |
| [Dostosuj model za pomocą fine-tuningu](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst) | Azure OpenAI Service pozwala dostosować nasze modele do Twoich osobistych zbiorów danych za pomocą fine-tuningu. Dowiedz się **jak wykonać fine-tuning (proces)** wybranych modeli za pomocą Azure AI Studio, Python SDK lub REST API.                                                                                       |
| [Zalecenia dotyczące fine-tuningu LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                      | LLM mogą nie działać dobrze w konkretnych domenach, zadaniach lub zbiorach danych, lub mogą generować niedokładne lub mylące wyniki. **Kiedy należy rozważyć fine-tuning** jako możliwe rozwiązanie tego problemu?                                                                                                           |
| [Ciągły Fine Tuning](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning?tabs=turbo%2Cpython&pivots=programming-language-studio#continuous-fine-tuning?WT.mc_id=academic-105485-koreyst)                    | Ciągły fine-tuning jest iteracyjnym procesem wybierania już dostrojonego modelu jako modelu bazowego i **dalszego dostrajania go** na nowych zestawach przykładów treningowych.                                                                                                                                              |
| [Fine-tuning i wywołania funkcji](https://learn.microsoft.com/azure/ai-services/openai/how-to/fine-tuning-functions?WT.mc_id=academic-105485-koreyst)                                                                           | Dostrajanie modelu **z przykładami wywołań funkcji** może poprawić wyniki modelu, uzyskując dokładniejsze i bardziej spójne wyniki - z podobnie sformatowanymi odpowiedziami i oszczędnością kosztów                                                                                                                         |
| [Modele Fine-tuning: Przewodnik Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#fine-tuning-models?WT.mc_id=academic-105485-koreyst)                                                         | Sprawdź tę tabelę, aby zrozumieć **które modele można dostrajać** w Azure OpenAI i w jakich regionach są dostępne. Sprawdź ich limity tokenów i daty wygaśnięcia danych treningowych, jeśli to konieczne.                                                                                                                    |
| [Dostrajać czy Nie Dostrajać? Oto Jest Pytanie](https://learn.microsoft.com/shows/ai-show/to-fine-tune-or-not-fine-tune-that-is-the-question?WT.mc_id=academic-105485-koreyst)                                                  | Ten 30-minutowy odcinek AI Show z **października 2023** omawia korzyści, wady i praktyczne wskazówki, które pomogą Ci podjąć tę decyzję.                                                                                                                                                                                     |
| [Pierwsze Kroki Z Fine-Tuningiem LLM](https://learn.microsoft.com/ai/playbook/technology-guidance/generative-ai/working-with-llms/fine-tuning-recommend?WT.mc_id=academic-105485-koreyst)                                       | Ten zasób z **AI Playbook** przeprowadzi Cię przez wymagania dotyczące danych, formatowanie, dostrajanie hiperparametrów oraz wyzwania/ograniczenia, o których powinieneś wiedzieć.                                                                                                                                          |
| **Samouczek**: [Azure OpenAI GPT3.5 Turbo Fine-Tuning](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python%2Ccommand-line?WT.mc_id=academic-105485-koreyst)                                    | Dowiedz się, jak utworzyć przykładowy zbiór danych do fine-tuningu, przygotować się do fine-tuningu, utworzyć zadanie fine-tuningu i wdrożyć dostrojony model w Azure.                                                                                                                                                       |
| **Samouczek**: [Dostrajanie modelu Llama 2 w Azure AI Studio](https://learn.microsoft.com/azure/ai-studio/how-to/fine-tune-model-llama?WT.mc_id=academic-105485-koreyst)                                                        | Azure AI Studio pozwala dostosować duże modele językowe do Twoich osobistych zbiorów danych _przy użyciu przepływu pracy opartego na interfejsie użytkownika, odpowiedniego dla programistów niskokodemokratycznych_. Zobacz ten przykład.                                                                                   |
| **Samouczek**:[Dostrajanie modeli Hugging Face dla pojedynczego GPU w Azure](https://learn.microsoft.com/azure/databricks/machine-learning/train-model/huggingface/fine-tune-model?WT.mc_id=academic-105485-koreyst)            | Ten artykuł opisuje, jak dostroić model Hugging Face za pomocą biblioteki transformers Hugging Face na pojedynczym GPU z Azure DataBricks + bibliotekami Hugging Face Trainer                                                                                                                                                |
| **Szkolenie:** [Dostosuj model podstawowy za pomocą Azure Machine Learning](https://learn.microsoft.com/training/modules/finetune-foundation-model-with-azure-machine-learning/?WT.mc_id=academic-105485-koreyst)               | Katalog modeli w Azure Machine Learning oferuje wiele modeli open source, które możesz dostosować do konkretnego zadania. Wypróbuj ten moduł [ze ścieżki edukacyjnej AzureML Generative AI](https://learn.microsoft.com/training/paths/work-with-generative-models-azure-machine-learning/?WT.mc_id=academic-105485-koreyst) |
| **Samouczek:** [Azure OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/azure-openai-fine-tuning?WT.mc_id=academic-105485-koreyst)                                                                                  | Dostrajanie modeli GPT-3.5 lub GPT-4 na Microsoft Azure przy użyciu W&B pozwala na szczegółowe śledzenie i analizę wydajności modelu. Ten przewodnik rozszerza koncepcje z przewodnika OpenAI Fine-Tuning o konkretne kroki i funkcje dla Azure OpenAI.                                                                      |
|                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                              |

## 2. Dodatkowe Zasoby

Ta sekcja zawiera dodatkowe zasoby, które warto odkryć, ale których nie mieliśmy czasu omówić w tej lekcji. Mogą zostać omówione w przyszłej lekcji lub jako opcja dodatkowego zadania w późniejszym terminie. Na razie używaj ich do budowania własnej wiedzy i umiejętności w tym temacie.

| Tytuł/Link                                                                                                                                                                                                         | Opis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI Cookbook**: [Przygotowanie i analiza danych do fine-tuningu modelu czatu](https://cookbook.openai.com/examples/chat_finetuning_data_prep?WT.mc_id=academic-105485-koreyst)                                | Ten notatnik służy jako narzędzie do wstępnego przetwarzania i analizy zbioru danych czatu używanego do dostrajania modelu czatu. Sprawdza błędy formatu, dostarcza podstawowe statystyki i szacuje liczbę tokenów dla kosztów fine-tuningu. Zobacz: [Metoda fine-tuningu dla gpt-3.5-turbo](https://platform.openai.com/docs/guides/fine-tuning?WT.mc_id=academic-105485-koreyst).                                                                                                                                                                                  |
| **OpenAI Cookbook**: [Fine-Tuning dla Retrieval Augmented Generation (RAG) z Qdrant](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant?WT.mc_id=academic-105485-koreyst) | Celem tego notatnika jest przeprowadzenie kompleksowego przykładu, jak dostroić modele OpenAI do Retrieval Augmented Generation (RAG). Będziemy również integrować Qdrant i Few-Shot Learning, aby zwiększyć wydajność modelu i zmniejszyć fabrykacje.                                                                                                                                                                                                                                                                                                               |
| **OpenAI Cookbook**: [Fine-tuning GPT z Weights & Biases](https://cookbook.openai.com/examples/third_party/gpt_finetuning_with_wandb?WT.mc_id=academic-105485-koreyst)                                             | Weights & Biases (W&B) to platforma dla deweloperów AI, z narzędziami do trenowania modeli, dostrajania modeli i wykorzystywania modeli podstawowych. Przeczytaj najpierw ich przewodnik [OpenAI Fine-Tuning](https://docs.wandb.ai/guides/integrations/openai-fine-tuning/?WT.mc_id=academic-105485-koreyst), a następnie wypróbuj ćwiczenie z Cookbook.                                                                                                                                                                                                            |
| **Samouczek Społeczności** [Phinetuning 2.0](https://huggingface.co/blog/g-ronimo/phinetuning?WT.mc_id=academic-105485-koreyst) - fine-tuning dla Małych Modeli Językowych                                         | Poznaj [Phi-2](https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/?WT.mc_id=academic-105485-koreyst), nowy mały model Microsoftu, niezwykle potężny, a jednocześnie kompaktowy. Ten samouczek przeprowadzi Cię przez dostrajanie Phi-2, pokazując, jak zbudować unikalny zbiór danych i dostroić model za pomocą QLoRA.                                                                                                                                                                                                     |
| **Samouczek Hugging Face** [Jak Wykonać Fine-Tune LLM w 2024 z Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                           | Ten wpis na blogu przeprowadzi Cię przez to, jak dostroić otwarte LLM za pomocą Hugging Face TRL, Transformers i zbiorów danych w 2024 roku. Definiujesz przypadek użycia, ustawiasz środowisko deweloperskie, przygotowujesz zbiór danych, dostrajasz model, testujesz i oceniasz go, a następnie wdrażasz do produkcji.                                                                                                                                                                                                                                            |
| **Hugging Face: [AutoTrain Advanced](https://github.com/huggingface/autotrain-advanced?WT.mc_id=academic-105485-koreyst)**                                                                                         | Zapewnia szybsze i łatwiejsze trenowanie i wdrażanie [najnowocześniejszych modeli uczenia maszynowego](https://twitter.com/abhi1thakur/status/1755167674894557291?WT.mc_id=academic-105485-koreyst). Repozytorium zawiera przyjazne dla Colab samouczki z przewodnikami wideo na YouTube do fine-tuningu. **Odzwierciedla niedawną aktualizację [local-first](https://twitter.com/abhi1thakur/status/1750828141805777057?WT.mc_id=academic-105485-koreyst)**. Przeczytaj [dokumentację AutoTrain](https://huggingface.co/autotrain?WT.mc_id=academic-105485-koreyst) |
|                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
